 You are given an array a. You need to find the shortest distance between two indexes. where distance refers to the sum of numbers between indexes from both the sides. 
-----Input----- The first line of the input contains two integers n and m (1 ≤ n, m ≤ 100 000) — the lengths of sequences f_{i} and b_{i} respectively. The second line contains n integers, determining sequence f_1, f_2, ..., f_{n} (1 ≤ f_{i} ≤ n). The last line contains m integers, determining sequence b_1, b_2, ..., b_{m} (1 ≤ b_{i} ≤ n). -----Output----- Print "Possible" if there is exactly one sequence a_{i}, such that b_{i} = f_{a}_{i} for all i from 1 to m. Then print m integers a_1, a_2, ..., a_{m}. If there are multiple suitable sequences a_{i}, print "Ambiguity". If Spongebob has made a mistake in his calculations and no suitable sequence a_{i} exists, print "Impossible". -----Examples----- Input 3 3 3 2 1 1 2 3 Output Possible 3 2 1  Input 3 3 1 1 1 1 1 1 Output Ambiguity Input 3 3 1 2 1 3 3 3 Output Impossible -----Note----- In the first sample 3 is replaced by 1 and vice versa, while 2 never changes. The answer exists and is unique. In the second sample all numbers are replaced by 1, so it is impossible to unambiguously restore the original sequence. In the third sample f_{i} ≠ 3 for all i, so no sequence a_{i} transforms into such b_{i} and we can say for sure that Spongebob has made a mistake.