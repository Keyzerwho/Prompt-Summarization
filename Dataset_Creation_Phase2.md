# Dataset Creation Phase 2
In this phase we used the API from [AI21 Studio](https://studio.ai21.com/playground) to generate summaries with their large (7B parameters) and jumbo (178B parameters).

## Code additions
In the `src` directory the file `api.py` will query the API with the config options specified in `config.yaml`. 
These options specify the following:
1. What model to use.
2. What prompt header to prepend to the input.
3. How many examples to prepend to the input. The examples are taken from examples we have already hand written.
4. The type of summary to use from our human examples. So `summary.txt` or `expert.txt`
5. Different API parameters such as temperature, topP, and max tokens.

Running this file will generate samples until the daily token limit has been reached for that API key.
Which is 30,000 and 10,000 for the large and jumbo models respectively.

## Summary additions
Using AI21's API we have generated 132 summaries. The breakdown is as follows.
- competition = 13 summaries
- interview = 48 summaries
- introductory = 71 summaries

The reason for the discrepancy summary amount is that we choose what to summarize randomly and there are more introductory problems. We did not change this
because it will allow us to gauge the model's performance more accurately. The introductory questions are easier for us to verify and for the model to summarize,
so if the model struggled there, then we need to change the parameters drastically. Next, we plan to only generate interview and competition questions because
we are interested if summaries in these categories will improve program synthesis success.

Along with the API generation we added more human made summaries. The more golden examples the better. The breakdown is as follows.
- competition = 3 summaries
- interview = 19 summaries
- introductory = 12 summaries

In total we have:
- competition = 25 summaries
- interview =  78 summaries
- introductory = 93 summaries
- total = 196 summaries


## Further Improvements
Not every summary generated by the model was verified by a human. This means there could be erroneous or incomplete summaries in the datset.
That's why we have put the samples generated by the model in a separate directory so we can test them specifically to catch any deficiencies.

Also, we plan to implement a web scraper to tell what type of question each prompt is. Each problem comes with a url and most websites will contain
some sort of provlem tags (i.e strings, graphs, binary search, ... ). If we know what type of problem we are asking to summarize
we can then use similar problems as examples for the input. By providing similar questions we hope to improve model accuracy.
