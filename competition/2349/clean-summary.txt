You are given a number of magnitude 1.You need to divide this number into as many positive parts as possible such that at any moment you can only divide one part into two smaller parts and the length of larger part may not be larger than the length of shortest one, times some constant factor.Input:One floating-point number, 1 ≤ k ≤ 1.999, meaning the stated constant factor. The number will have at most 3 digits after the decimal point.Output:First, you should output one number n, the maximal achievable number of parts for the given value of the constant factor. Then, you should output any proof that this number of parts is in fact achievable: n-1 descriptions of dividing, using the following notation. At each step, you print two numbers: first, the index of the part that you want to divide into two; second, the length of the new number formed. It is assumed that the starting number has index 0. Each newly created number will be given the lowest possible free integer index (so, at the ith step this will be i). Each time, the size of number will be decreased by the size of the newly created number.-----Example-----Input:1.5Output:40 0.40 0.31 0.2